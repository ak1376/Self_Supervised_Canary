{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"/home/akapoor/Dropbox (University of Oregon)/Kapoor_Ananya/01_Projects/01_b_Canary_SSL/Canary_SSL_Repo/Num_Spectrograms_10_Window_Size_100_Stride_10\" already exists.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import umap\n",
    "import warnings\n",
    "from bokeh.plotting import figure, show, output_file, save\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set parameters\n",
    "bird_dir = '/home/akapoor/Dropbox (University of Oregon)/Kapoor_Ananya/01_Projects/01_a_Rotations/Gardner_Lab/Canary_Data/llb3/'\n",
    "audio_files = bird_dir+'llb3_songs'\n",
    "directory = bird_dir+ 'llb3_data_matrices/Python_Files'\n",
    "analysis_path = '/home/akapoor/Dropbox (University of Oregon)/Kapoor_Ananya/01_Projects/01_b_Canary_SSL/Canary_SSL_Repo/'\n",
    "\n",
    "# Parameters we set\n",
    "num_spec = 10\n",
    "window_size = 100\n",
    "stride = 10\n",
    "\n",
    "# Define the folder name\n",
    "folder_name = f'{analysis_path}Num_Spectrograms_{num_spec}_Window_Size_{window_size}_Stride_{stride}'\n",
    "\n",
    "# Create the folder if it doesn't already exist\n",
    "if not os.path.exists(folder_name+\"/Plots/Window_Plots\"):\n",
    "    os.makedirs(folder_name+\"/Plots/Window_Plots\")\n",
    "    print(f'Folder \"{folder_name}\" created successfully.')\n",
    "else:\n",
    "    print(f'Folder \"{folder_name}\" already exists.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# # If you are loading the results from a previous analysis, run the following lines of code\n",
    "# =============================================================================\n",
    "\n",
    "stacked_windows = np.load(folder_name+'/stacked_windows.npy') # An array of all the mini-spectrograms\n",
    "stacked_labels_for_window = np.load(folder_name+'/stacked_labels_for_window.npy') # The syllable labels for each time point in each mini-spectrogram\n",
    "embedding = np.load(folder_name+'/UMAP_Embedding.npy') # The pre-computed UMAP embedding (2 dimensional)\n",
    "masked_frequencies = np.load(analysis_path+'/masked_frequencies_lowthresh_500_highthresh_7000.npy') # The frequencies we want to use for analysis. Excluding unnecessarily low and high frequencies\n",
    "stacked_window_times = np.load(folder_name+'/stacked_window_times.npy') # The onsets and ending of each mini-spectrogram\n",
    "    \n",
    "# open the file for reading in binary mode\n",
    "with open(folder_name+'/category_colors.pkl', 'rb') as f:\n",
    "    # load the dictionary from the file using pickle.load()\n",
    "    category_colors = pickle.load(f)   \n",
    "    \n",
    "# Each syllable is given a unique color. Each mini-spectrogram will have an average syllable color associated with it. This is the average RGB value across all unique syllables in the mini-spectrogram\n",
    "mean_colors_per_minispec = np.load(folder_name+'/mean_colors_per_minispec.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# # If you're running the analysis for the first time \n",
    "# =============================================================================\n",
    "\n",
    "files = os.listdir(directory)\n",
    "all_songs_data = [element for element in files if '.npz' in element] # Get the file paths of each numpy file from Yarden's data\n",
    "all_songs_data.sort()\n",
    "os.chdir(directory)\n",
    "\n",
    "# For each spectrogram we will extract\n",
    "# 1. Each timepoint's syllable label\n",
    "# 2. The spectrogram itself\n",
    "stacked_labels = [] \n",
    "stacked_specs = []\n",
    "for i in np.arange(num_spec):\n",
    "    # Extract the data within the numpy file. We will use this to create the spectrogram\n",
    "    dat = np.load(all_songs_data[i])\n",
    "    spec = dat['s']\n",
    "    times = dat['t']\n",
    "    frequencies = dat['f']\n",
    "    labels = dat['labels']\n",
    "    labels = labels.T\n",
    "\n",
    "\n",
    "    # Let's get rid of higher order frequencies\n",
    "    mask = (frequencies<7000)&(frequencies>500)\n",
    "    masked_frequencies = frequencies[mask]\n",
    "\n",
    "    subsetted_spec = spec[mask.reshape(mask.shape[0],),:]\n",
    "    \n",
    "    stacked_labels.append(labels)\n",
    "    stacked_specs.append(subsetted_spec)\n",
    "\n",
    "    \n",
    "stacked_specs = np.concatenate((stacked_specs), axis = 1)\n",
    "stacked_labels = np.concatenate((stacked_labels), axis = 0)\n",
    "\n",
    "# Get a list of unique categories (syllable labels)\n",
    "unique_categories = np.unique(stacked_labels)\n",
    "\n",
    "# Create a dictionary that maps categories to random colors\n",
    "category_colors = {category: np.random.rand(3,) for category in unique_categories}\n",
    "\n",
    "spec_for_analysis = stacked_specs.T\n",
    "window_labels_arr = []\n",
    "embedding_arr = []\n",
    "# Find the exact sampling frequency (the time in miliseconds between one pixel [timepoint] and another pixel)\n",
    "dx = np.diff(times)[0,0]\n",
    "\n",
    "# We will now extract each mini-spectrogram from the full spectrogram\n",
    "stacked_windows = []\n",
    "# Find the syllable labels for each mini-spectrogram\n",
    "stacked_labels_for_window = []\n",
    "# Find the mini-spectrograms onset and ending times \n",
    "stacked_window_times = []\n",
    "\n",
    "# The below for-loop will find each mini-spectrogram (window) and populate the empty lists we defined above.\n",
    "for i in range(0, spec_for_analysis.shape[0] - window_size + 1, stride):\n",
    "    # Find the window\n",
    "    window = spec_for_analysis[i:i + window_size, :]\n",
    "    # Get the window onset and ending times\n",
    "    window_times = dx*np.arange(i, i + window_size)\n",
    "    # We will flatten the window to be a 1D vector\n",
    "    window = window.reshape(1, window.shape[0]*window.shape[1])\n",
    "    # Extract the syllable labels for the window\n",
    "    labels_for_window = stacked_labels[i:i+window_size, :]\n",
    "    # Reshape the syllable labels for the window into a 1D array\n",
    "    labels_for_window = labels_for_window.reshape(1, labels_for_window.shape[0]*labels_for_window.shape[1])\n",
    "    # Populate the empty lists defined above\n",
    "    stacked_windows.append(window)\n",
    "    stacked_labels_for_window.append(labels_for_window)\n",
    "    stacked_window_times.append(window_times)\n",
    "\n",
    "# Convert the populated lists into a stacked numpy array\n",
    "stacked_windows = np.stack(stacked_windows, axis = 0)\n",
    "stacked_windows = np.squeeze(stacked_windows)\n",
    "\n",
    "stacked_labels_for_window = np.stack(stacked_labels_for_window, axis = 0)\n",
    "stacked_labels_for_window = np.squeeze(stacked_labels_for_window)\n",
    "\n",
    "stacked_window_times = np.stack(stacked_window_times, axis = 0)\n",
    "\n",
    "# For each mini-spectrogram, find the average color across all unique syllables\n",
    "mean_colors_per_minispec = np.zeros((stacked_labels_for_window.shape[0], 3))\n",
    "for i in np.arange(stacked_labels_for_window.shape[0]):\n",
    "    list_of_colors_for_row = [category_colors[x] for x in stacked_labels_for_window[i,:]]\n",
    "    all_colors_in_minispec = np.array(list_of_colors_for_row)\n",
    "    mean_color = np.mean(all_colors_in_minispec, axis = 0)\n",
    "    mean_colors_per_minispec[i,:] = mean_color\n",
    "    \n",
    "# Perform a UMAP embedding on the dataset of mini-spectrograms\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(stacked_windows)\n",
    "\n",
    "# Let's save all the numpy arrays\n",
    "np.save(folder_name+'/stacked_windows.npy', stacked_windows)\n",
    "np.save(folder_name+'/stacked_labels_for_window.npy', stacked_labels_for_window)\n",
    "np.save(analysis_path+'/masked_frequencies_lowthresh_500_highthresh_7000.npy', masked_frequencies)\n",
    "np.save(folder_name+'/stacked_window_times.npy', stacked_window_times)\n",
    "np.save(folder_name+'/mean_colors_per_minispec.npy', mean_colors_per_minispec)\n",
    "\n",
    "# open a file for writing in binary mode\n",
    "with open(folder_name+'/category_colors.pkl', 'wb') as f:\n",
    "    # write the dictionary to the file using pickle.dump()\n",
    "    pickle.dump(category_colors, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below function will save an image for each mini-spectrogram. This will be used for understanding the UMAP plot.\n",
    "def embeddable_image(data, window_times, iteration_number):\n",
    "    \n",
    "    data.shape = (window_size, int(data.shape[0]/window_size))\n",
    "    data = data.T \n",
    "    window_times = window_times.reshape(1, window_times.shape[0])\n",
    "    plt.pcolormesh(window_times, masked_frequencies, data, cmap='jet')\n",
    "    # let's save the plt colormesh as an image.\n",
    "    plt.savefig(folder_name+'/Plots/Window_Plots/'+f'Window_{iteration_number}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "for i in np.arange(stacked_windows.shape[0]):\n",
    "    if i%10 == 0:\n",
    "        print(f'Iteration {i} of {stacked_windows.shape[0]}')\n",
    "    data = stacked_windows[i,:]\n",
    "    window_times = stacked_window_times[i,:]\n",
    "    embeddable_image(data, window_times, i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a UMAP embedding on the dataset of mini-spectrograms\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(stacked_windows)\n",
    "np.save(folder_name+'/UMAP_Embedding.npy', embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below function will save an image for each mini-spectrogram. This will be used for understanding the UMAP plot.\n",
    "def embeddable_image(data, window_times, iteration_number):\n",
    "    \n",
    "    data.shape = (window_size, int(data.shape[0]/window_size))\n",
    "    data = data.T \n",
    "    window_times = window_times.reshape(1, window_times.shape[0])\n",
    "    plt.pcolormesh(window_times, masked_frequencies, data, cmap='jet')\n",
    "    # let's save the plt colormesh as an image.\n",
    "    plt.savefig(folder_name+'/Plots/Window_Plots/'+f'Window_{iteration_number}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "for i in np.arange(stacked_windows.shape[0]):\n",
    "    if i%10 == 0:\n",
    "        print(f'Iteration {i} of {stacked_windows.shape[0]}')\n",
    "    data = stacked_windows[i,:]\n",
    "    window_times = stacked_window_times[i,:]\n",
    "    embeddable_image(data, window_times, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/akapoor/Dropbox (University of Oregon)/Kapoor_Ananya/01_Projects/01_b_Canary_SSL/Canary_SSL_Repo/Num_Spectrograms_10_Window_Size_100_Stride_10/Plots/umap.html'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify an HTML file to save the Bokeh image to.\n",
    "output_file(filename=f'{folder_name}/Plots/umap.html')\n",
    "\n",
    "# Convert the UMAP embedding to a Pandas Dataframe\n",
    "spec_df = pd.DataFrame(embedding, columns=('x', 'y'))\n",
    "\n",
    "\n",
    "# Create a ColumnDataSource from the data. This contains the UMAP embedding components and the mean colors per mini-spectrogram\n",
    "source = ColumnDataSource(data=dict(x = embedding[:,0], y = embedding[:,1], colors=mean_colors_per_minispec))\n",
    "\n",
    "\n",
    "# Create a figure and add a scatter plot\n",
    "p = figure(width=800, height=600, tools=('pan, box_zoom, hover, reset'))\n",
    "p.scatter(x='x', y='y', size = 7, color = 'colors', source=source)\n",
    "\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = \"\"\"\n",
    "    <div>\n",
    "        <h3>@x, @y</h3>\n",
    "        <div>\n",
    "            <img\n",
    "                src=\"@image\" height=\"100\" alt=\"@image\" width=\"100\"\n",
    "                style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                border=\"2\"\n",
    "            ></img>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=\"\"\"\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# Set the image path for each data point\n",
    "source.data['image'] = []\n",
    "for i in np.arange(spec_df.shape[0]):\n",
    "    source.data['image'].append(f'{folder_name}/Plots/Window_Plots/Window_{i}.png')\n",
    "\n",
    "show(p)\n",
    "\n",
    "save(p)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Contrastive Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = \"cuda:0\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "\n",
    "# Let's shuffle the stacked_windows and stacked_labels_for_window\n",
    "\n",
    "# Shuffle the indices of stacked_windows\n",
    "shuffled_indices = np.random.permutation(stacked_windows.shape[0])\n",
    "\n",
    "# Shuffle array1 using the shuffled indices\n",
    "stacked_windows_NCE = stacked_windows[shuffled_indices,:]\n",
    "\n",
    "# Shuffle array2 using the same shuffled indices\n",
    "stacked_labels_for_window_NCE = stacked_labels_for_window[shuffled_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define neural network\n",
    "\n",
    "class TweetyNetCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=32, kernel_size=(5, 5), stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(8, 1), stride=(8, 1))\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=(5, 5), stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(8, 1), stride=(8, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(9408, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        # x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "cnn_model = TweetyNetCNN()\n",
    "cnn_model = cnn_model.double()\n",
    "# cnn_model = cnn_model.double().to(device)\n",
    "\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_windows_tensor = stacked_windows_NCE\n",
    "stacked_windows_tensor.shape = (stacked_windows_NCE.shape[0], 1, 100, 151)\n",
    "stacked_windows_tensor = torch.tensor(stacked_windows_NCE).double()\n",
    "\n",
    "\n",
    "actual_labels = np.max(stacked_labels_for_window_NCE, axis=1)\n",
    "actual_labels = torch.tensor(actual_labels)\n",
    "\n",
    "# Determine the split point for testing data\n",
    "split_point = int(0.7 * stacked_windows_tensor.shape[0])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = stacked_windows_tensor[:split_point,:,:,:], stacked_windows_tensor[split_point:,:,:,:]\n",
    "train_labels, test_labels = actual_labels[:split_point], actual_labels[split_point:]\n",
    "\n",
    "# Convert the numpy arrays to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(train_data)\n",
    "test_data_tensor = torch.tensor(test_data)\n",
    "train_labels_tensor = torch.tensor(train_labels)  # Assuming integer labels\n",
    "test_labels_tensor = torch.tensor(test_labels)\n",
    "\n",
    "# Create TensorDatasets for training and testing sets\n",
    "train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "\n",
    "# Define batch size for DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle test data\n",
    "\n",
    "\n",
    "# Choose a random 64 indices from our total data\n",
    "\n",
    "# batch_indices = np.random.randint(0, stacked_windows_tensor.shape[0], batch_size)\n",
    "\n",
    "# x = stacked_windows_tensor[batch_indices, :,:,:]\n",
    "# y = actual_labels[batch_indices]\n",
    "\n",
    "# # Create a TensorDataset to combine features and labels\n",
    "# dataset = TensorDataset(stacked_windows_tensor, actual_labels)\n",
    "\n",
    "# train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Extract the first batch\n",
    "# for batch in train_loader:\n",
    "#     first_batch = batch\n",
    "#     break  # Exit the loop after the first iteration\n",
    "\n",
    "# x = first_batch[0]\n",
    "# y = first_batch[1]\n",
    "\n",
    "positive_sample_index_list = []\n",
    "negative_samples_indices_list = []\n",
    "\n",
    "\n",
    "unique_syllables = torch.unique(actual_labels)\n",
    "\n",
    "indices_dict = {int(element): np.where(actual_labels == element)[0] for element in unique_syllables}\n",
    "num_negative_samples_each = 5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the embedding from the non-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = cnn_model.to('cpu')\n",
    "model_embedding_arr_untrained = torch.empty((0,1000)).to('cpu')\n",
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    x = cnn_model.conv1(data.to('cpu'))\n",
    "    x = cnn_model.relu(x)\n",
    "    x = cnn_model.pool1(x)\n",
    "    x = cnn_model.conv2(x)\n",
    "    x = cnn_model.relu(x)\n",
    "    x = cnn_model.pool2(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = cnn_model.fc1(x)\n",
    "    model_embedding_arr_untrained = torch.concatenate((model_embedding_arr_untrained, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.054430GB\n",
      "torch.cuda.memory_reserved: 20.580078GB\n",
      "torch.cuda.max_memory_reserved: 20.580078GB\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "umap_embedding_untrained = reducer.fit_transform(model_embedding_arr_untrained.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_colors_per_minispec_training = mean_colors_per_minispec[shuffled_indices[:split_point],:]\n",
    "mean_colors_per_minispec_testing = mean_colors_per_minispec[shuffled_indices[split_point:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/akapoor/Dropbox (University of Oregon)/Kapoor_Ananya/01_Projects/01_b_Canary_SSL/Canary_SSL_Repo/Num_Spectrograms_10_Window_Size_100_Stride_10/Plots/umap_of_TweetyNet_Embed_NCE_untrained.html'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify an HTML file to save the Bokeh image to.\n",
    "output_file(filename=f'{folder_name}/Plots/umap_of_TweetyNet_Embed_NCE_untrained.html')\n",
    "\n",
    "# Convert the UMAP embedding to a Pandas Dataframe\n",
    "spec_df = pd.DataFrame(umap_embedding_untrained, columns=('x', 'y'))\n",
    "\n",
    "\n",
    "# Create a ColumnDataSource from the data. This contains the UMAP embedding components and the mean colors per mini-spectrogram\n",
    "source = ColumnDataSource(data=dict(x = umap_embedding_untrained[:,0], y = umap_embedding_untrained[:,1], colors=mean_colors_per_minispec_training))\n",
    "\n",
    "\n",
    "# Create a figure and add a scatter plot\n",
    "p = figure(width=800, height=600, tools=('pan, box_zoom, hover, reset'))\n",
    "p.scatter(x='x', y='y', size = 7, color = 'colors', source=source)\n",
    "\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = \"\"\"\n",
    "    <div>\n",
    "        <h3>@x, @y</h3>\n",
    "        <div>\n",
    "            <img\n",
    "                src=\"@image\" height=\"100\" alt=\"@image\" width=\"100\"\n",
    "                style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                border=\"2\"\n",
    "            ></img>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=\"\"\"\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# Set the image path for each data point\n",
    "source.data['image'] = []\n",
    "for i in np.arange(spec_df.shape[0]):\n",
    "    source.data['image'].append(f'{folder_name}/Plots/Window_Plots/Window_{shuffled_indices[:split_point][i]}.png')\n",
    "\n",
    "show(p)\n",
    "save(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 1, 100, 151])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = TweetyNetCNN()\n",
    "cnn_model = cnn_model.double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Epoch 0, Mean Batch Loss: 20.9646\n",
      "=================================================================\n",
      "=================================================================\n",
      "Epoch 1, Mean Batch Loss: 20.9647\n",
      "=================================================================\n",
      "=================================================================\n",
      "Epoch 2, Mean Batch Loss: 20.9646\n",
      "=================================================================\n",
      "=================================================================\n",
      "Epoch 3, Mean Batch Loss: 20.9646\n",
      "=================================================================\n",
      "=================================================================\n",
      "Epoch 4, Mean Batch Loss: 20.9647\n",
      "=================================================================\n",
      "=================================================================\n",
      "Epoch 5, Mean Batch Loss: 20.9648\n",
      "=================================================================\n",
      "=================================================================\n",
      "Epoch 6, Mean Batch Loss: 20.9645\n",
      "=================================================================\n",
      "=================================================================\n",
      "Epoch 7, Mean Batch Loss: 20.9646\n",
      "=================================================================\n",
      "=================================================================\n",
      "Epoch 8, Mean Batch Loss: 20.9644\n",
      "=================================================================\n",
      "=================================================================\n",
      "Epoch 9, Mean Batch Loss: 20.9647\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# %% Train Neural network \n",
    "\n",
    "cnn_model.train()\n",
    "cnn_model = cnn_model.to(device)\n",
    "batch_size = 64\n",
    "num_epoch = 10\n",
    "mean_batch_loss_per_epoch_list = []\n",
    "for epoch in np.arange(num_epoch):\n",
    "    total_batch_loss_list = []\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        batch_loss = 0\n",
    "        for anchor_index in np.arange(data.shape[0]):\n",
    "            anchor_label = targets[anchor_index]\n",
    "            # Sample a positive sample from our total dataset\n",
    "            indices_of_positve_samples = torch.where(actual_labels == anchor_label)[0]\n",
    "            positive_sample_index = torch.randint(0, indices_of_positve_samples.shape[0], size=(1,))\n",
    "            positive_sample_index_list.append(positive_sample_index.item())\n",
    "\n",
    "            # Sample negative samples from our total dataset. We will use a \n",
    "            # weighted probability distribution. There will be a 0.0001 probability\n",
    "            # that we will sample from the positive class and a (1-0.001)/(K-1)\n",
    "            # probability that we sample from the remaining K-1 classes\n",
    "\n",
    "            # Create a tensor with custom probabilities\n",
    "            epsilon = 0.0001\n",
    "            probs = torch.zeros((1, unique_syllables.shape[0]))\n",
    "            probs[:,:] = (1-epsilon)/(probs.shape[1]-1)\n",
    "            index_of_anchor_label = torch.where(unique_syllables == anchor_label)\n",
    "            probs[:,int(index_of_anchor_label[0])] = epsilon\n",
    "\n",
    "\n",
    "            # Number of samples to generate\n",
    "            num_samples = unique_syllables.shape[0] - 1\n",
    "\n",
    "            # Sample indices based on the custom probabilities\n",
    "            sampled_labels = torch.multinomial(probs, num_samples, replacement=False)\n",
    "\n",
    "            # Now let's randomly sample an index value from each sampled label\n",
    "\n",
    "            random_samples = {key: np.random.choice(values, num_negative_samples_each) for key, values in indices_dict.items() if key in sampled_labels}\n",
    "            indices_of_negative_samples = np.array(list(random_samples.values()))\n",
    "            negative_samples_indices_list.append(indices_of_negative_samples)\n",
    "            a = np.stack(indices_of_negative_samples)\n",
    "            indices_of_negative_samples = np.stack(indices_of_negative_samples).reshape(a.shape[0]*a.shape[1],)\n",
    "\n",
    "\n",
    "            # Now let's extract the positive and negative samples' spectrogram \n",
    "            # slices\n",
    "\n",
    "            positive_sample = stacked_windows_tensor[positive_sample_index, :,:,:]\n",
    "            negative_samples = stacked_windows_tensor[indices_of_negative_samples, :,:,:]\n",
    "\n",
    "            dat = torch.concatenate((positive_sample, negative_samples))\n",
    "\n",
    "            artificial_labels = torch.zeros((1,1 + (unique_syllables.shape[0]-1)*num_negative_samples_each))\n",
    "            artificial_labels[:,0] = 1\n",
    "\n",
    "            # Get the number of rows in the tensors\n",
    "            num_rows = dat.shape[0]\n",
    "\n",
    "            # Generate a random permutation of indices\n",
    "            shuffled_indices_pseudo = torch.randperm(num_rows)\n",
    "\n",
    "            # Shuffle both tensors based on the same indices\n",
    "            dat = dat[shuffled_indices_pseudo,:,:,:].to(device)\n",
    "            artificial_labels = artificial_labels[:,shuffled_indices_pseudo].to(device)\n",
    "\n",
    "            pred_probs = cnn_model(dat)\n",
    "\n",
    "            # h_t = 1/(1+torch.exp(-1*(torch.log(pred_probs) - torch.log(probs).T)))\n",
    "            # torch.sum(artificial_labels.T*torch.log(h_t) + (1-artificial_labels.T)*torch.log(h_t))\n",
    "\n",
    "            loss = criterion(pred_probs, artificial_labels.T.double())\n",
    "            batch_loss+=loss\n",
    "\n",
    "        total_batch_loss_list.append(batch_loss.item())\n",
    "        # print(f'Batch Loss: {batch_loss.item():.4f}')\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    mean_batch_loss_per_epoch = np.mean(total_batch_loss_list)\n",
    "    mean_batch_loss_per_epoch_list.append(mean_batch_loss_per_epoch)\n",
    "    print(\"=================================================================\")\n",
    "    print(f'Epoch {epoch}, Mean Batch Loss: {mean_batch_loss_per_epoch:.4f}')\n",
    "    print(\"=================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  19565 MiB |  20122 MiB |  34945 MiB |  15379 MiB |\\n|       from large pool |  19561 MiB |  20118 MiB |  34919 MiB |  15357 MiB |\\n|       from small pool |      3 MiB |      4 MiB |     26 MiB |     22 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  19565 MiB |  20122 MiB |  34945 MiB |  15379 MiB |\\n|       from large pool |  19561 MiB |  20118 MiB |  34919 MiB |  15357 MiB |\\n|       from small pool |      3 MiB |      4 MiB |     26 MiB |     22 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  19565 MiB |  20121 MiB |  34945 MiB |  15379 MiB |\\n|       from large pool |  19561 MiB |  20118 MiB |  34919 MiB |  15357 MiB |\\n|       from small pool |      3 MiB |      4 MiB |     26 MiB |     22 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  21066 MiB |  21068 MiB |  21068 MiB |   2048 KiB |\\n|       from large pool |  21062 MiB |  21062 MiB |  21062 MiB |      0 KiB |\\n|       from small pool |      4 MiB |      6 MiB |      6 MiB |   2048 KiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   1500 MiB |   1718 MiB |   9333 MiB |   7833 MiB |\\n|       from large pool |   1500 MiB |   1716 MiB |   9307 MiB |   7807 MiB |\\n|       from small pool |      0 MiB |      1 MiB |     25 MiB |     25 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     136    |     137    |     223    |      87    |\\n|       from large pool |      91    |      91    |     113    |      22    |\\n|       from small pool |      45    |      47    |     110    |      65    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     136    |     137    |     223    |      87    |\\n|       from large pool |      91    |      91    |     113    |      22    |\\n|       from small pool |      45    |      47    |     110    |      65    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      22    |      23    |      23    |       1    |\\n|       from large pool |      20    |      20    |      20    |       0    |\\n|       from small pool |       2    |       3    |       3    |       1    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      27    |      29    |      83    |      56    |\\n|       from large pool |      25    |      25    |      36    |      11    |\\n|       from small pool |       2    |       4    |      47    |      45    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.to('cpu').eval()\n",
    "model_embedding_arr = torch.empty((0,1000)).to('cpu')\n",
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    x = cnn_model.conv1(data.to('cpu'))\n",
    "    x = cnn_model.relu(x)\n",
    "    x = cnn_model.pool1(x)\n",
    "    x = cnn_model.conv2(x)\n",
    "    x = cnn_model.relu(x)\n",
    "    x = cnn_model.pool2(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = cnn_model.fc1(x)\n",
    "    model_embedding_arr = torch.concatenate((model_embedding_arr, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "embedding_umap = reducer.fit_transform(model_embedding_arr.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_colors_per_minispec_training = mean_colors_per_minispec[shuffled_indices[:split_point],:]\n",
    "mean_colors_per_minispec_testing = mean_colors_per_minispec[shuffled_indices[split_point:],:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP Representation of Untrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/akapoor/Dropbox (University of Oregon)/Kapoor_Ananya/01_Projects/01_b_Canary_SSL/Canary_SSL_Repo/Num_Spectrograms_10_Window_Size_100_Stride_10/Plots/umap_of_TweetyNet_Embed_NCE_trained.html'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify an HTML file to save the Bokeh image to.\n",
    "output_file(filename=f'{folder_name}/Plots/umap_of_TweetyNet_Embed_NCE_trained.html')\n",
    "\n",
    "# Convert the UMAP embedding to a Pandas Dataframe\n",
    "spec_df = pd.DataFrame(embedding_umap, columns=('x', 'y'))\n",
    "\n",
    "\n",
    "# Create a ColumnDataSource from the data. This contains the UMAP embedding components and the mean colors per mini-spectrogram\n",
    "source = ColumnDataSource(data=dict(x = embedding_umap[:,0], y = embedding_umap[:,1], colors=mean_colors_per_minispec_training))\n",
    "\n",
    "\n",
    "# Create a figure and add a scatter plot\n",
    "p = figure(width=800, height=600, tools=('pan, box_zoom, hover, reset'))\n",
    "p.scatter(x='x', y='y', size = 7, color = 'colors', source=source)\n",
    "\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = \"\"\"\n",
    "    <div>\n",
    "        <h3>@x, @y</h3>\n",
    "        <div>\n",
    "            <img\n",
    "                src=\"@image\" height=\"100\" alt=\"@image\" width=\"100\"\n",
    "                style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                border=\"2\"\n",
    "            ></img>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=\"\"\"\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# Set the image path for each data point\n",
    "source.data['image'] = []\n",
    "for i in np.arange(spec_df.shape[0]):\n",
    "    source.data['image'].append(f'{folder_name}/Plots/Window_Plots/Window_{shuffled_indices[:split_point][i]}.png')\n",
    "\n",
    "show(p)\n",
    "save(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/akapoor/Dropbox (University of Oregon)/Kapoor_Ananya/01_Projects/01_b_Canary_SSL/Canary_SSL_Repo/Num_Spectrograms_10_Window_Size_100_Stride_10'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an HTML file to save the Bokeh image to.\n",
    "output_file(filename='umap_of_TweetyNet_Embed_NCE.html')\n",
    "\n",
    "# Convert the UMAP embedding to a Pandas Dataframe\n",
    "spec_df = pd.DataFrame(embedding_umap, columns=('x', 'y'))\n",
    "\n",
    "\n",
    "# Create a ColumnDataSource from the data. This contains the UMAP embedding components and the mean colors per mini-spectrogram\n",
    "source = ColumnDataSource(data=dict(x = embedding_umap[:,0], y = embedding_umap[:,1], colors=mean_colors_per_minispec_training))\n",
    "\n",
    "\n",
    "# Create a figure and add a scatter plot\n",
    "p = figure(width=800, height=600, tools=('pan, box_zoom, hover, reset'))\n",
    "p.scatter(x='x', y='y', size = 7, color = 'colors', source=source)\n",
    "\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = \"\"\"\n",
    "    <div>\n",
    "        <h3>@x, @y</h3>\n",
    "        <div>\n",
    "            <img\n",
    "                src=\"@image\" height=\"100\" alt=\"@image\" width=\"100\"\n",
    "                style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                border=\"2\"\n",
    "            ></img>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=\"\"\"\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# Set the image path for each data point\n",
    "source.data['image'] = []\n",
    "for i in np.arange(spec_df.shape[0]):\n",
    "    source.data['image'].append(f'{folderpath_song}/Plots/Window_Plots/Window_{shuffled_indices[split_point:]}.png')\n",
    "\n",
    "show(p)\n",
    "\n",
    "# save(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
