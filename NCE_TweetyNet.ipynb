{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Parameters we set\n",
    "num_spec = 10\n",
    "window_size = 100\n",
    "stride = 10\n",
    "\n",
    "\n",
    "phrase_repeats = 5\n",
    "num_songs = 10\n",
    "radius_value = 0.01\n",
    "num_syllables = 10\n",
    "\n",
    "folderpath = '/home/akapoor/Dropbox (University of Oregon)/Kapoor_Ananya/01_Projects/01_b_Canary_SSL/Canary_SSL_Repo/'\n",
    "folderpath_song = f'{folderpath}num_songs_{num_songs}_num_syllables_{num_syllables}_phrase_repeats_{phrase_repeats}_radius_{radius_value}/'\n",
    "\n",
    "mean_colors_per_minispec = np.load(f'{folderpath_song}mean_colors_per_minispec.npy')\n",
    "\n",
    "stacked_windows = np.load(f'{folderpath_song}stacked_windows.npy')\n",
    "# stacked_window_times = np.load(f'{folderpath_song}stacked_window_times.npy')\n",
    "stacked_labels_for_window = np.load(f'{folderpath_song}labels_for_window.npy')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda:0\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "\n",
    "# Let's shuffle the stacked_windows and stacked_labels_for_window\n",
    "\n",
    "# Shuffle the indices of stacked_windows\n",
    "shuffled_indices = np.random.permutation(stacked_windows.shape[0])\n",
    "\n",
    "# Shuffle array1 using the shuffled indices\n",
    "stacked_windows = stacked_windows[shuffled_indices,:]\n",
    "\n",
    "# Shuffle array2 using the same shuffled indices\n",
    "stacked_labels_for_window = stacked_labels_for_window[shuffled_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define neural network\n",
    "\n",
    "class TweetyNetCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=32, kernel_size=(5, 5), stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(8, 1), stride=(8, 1))\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=(5, 5), stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(8, 1), stride=(8, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(64*1*36, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        # x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "cnn_model = TweetyNetCNN()\n",
    "cnn_model = cnn_model.double().to(device)\n",
    "\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19394/3920792282.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data_tensor = torch.tensor(train_data)\n",
      "/tmp/ipykernel_19394/3920792282.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data_tensor = torch.tensor(test_data)\n",
      "/tmp/ipykernel_19394/3920792282.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels_tensor = torch.tensor(train_labels)  # Assuming integer labels\n",
      "/tmp/ipykernel_19394/3920792282.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_labels_tensor = torch.tensor(test_labels)\n"
     ]
    }
   ],
   "source": [
    "stacked_windows_tensor = stacked_windows.copy()\n",
    "stacked_windows_tensor.shape = (stacked_windows.shape[0], 1, 100, 40)\n",
    "stacked_windows_tensor = torch.tensor(stacked_windows_tensor).double()\n",
    "\n",
    "\n",
    "actual_labels = np.max(stacked_labels_for_window, axis=1)\n",
    "actual_labels = torch.tensor(actual_labels)\n",
    "\n",
    "# Determine the split point for testing data\n",
    "split_point = int(0.7 * stacked_windows_tensor.shape[0])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = stacked_windows_tensor[:split_point,:,:,:], stacked_windows_tensor[split_point:,:,:,:]\n",
    "train_labels, test_labels = actual_labels[:split_point], actual_labels[split_point:]\n",
    "\n",
    "# Convert the numpy arrays to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(train_data)\n",
    "test_data_tensor = torch.tensor(test_data)\n",
    "train_labels_tensor = torch.tensor(train_labels)  # Assuming integer labels\n",
    "test_labels_tensor = torch.tensor(test_labels)\n",
    "\n",
    "# Create TensorDatasets for training and testing sets\n",
    "train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "\n",
    "# Define batch size for DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle test data\n",
    "\n",
    "\n",
    "# Choose a random 64 indices from our total data\n",
    "\n",
    "# batch_indices = np.random.randint(0, stacked_windows_tensor.shape[0], batch_size)\n",
    "\n",
    "# x = stacked_windows_tensor[batch_indices, :,:,:]\n",
    "# y = actual_labels[batch_indices]\n",
    "\n",
    "# # Create a TensorDataset to combine features and labels\n",
    "# dataset = TensorDataset(stacked_windows_tensor, actual_labels)\n",
    "\n",
    "# train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Extract the first batch\n",
    "# for batch in train_loader:\n",
    "#     first_batch = batch\n",
    "#     break  # Exit the loop after the first iteration\n",
    "\n",
    "# x = first_batch[0]\n",
    "# y = first_batch[1]\n",
    "\n",
    "positive_sample_index_list = []\n",
    "negative_samples_indices_list = []\n",
    "\n",
    "\n",
    "unique_syllables = torch.unique(actual_labels)\n",
    "\n",
    "indices_dict = {int(element): np.where(actual_labels == element)[0] for element in unique_syllables}\n",
    "num_negative_samples_each = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Epoch 0, Mean Batch Loss: 2.2678\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# %% Train Neural network \n",
    "\n",
    "cnn_model.train()\n",
    "num_epoch = 1\n",
    "mean_batch_loss_per_epoch_list = []\n",
    "for epoch in np.arange(num_epoch):\n",
    "    total_batch_loss_list = []\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        batch_loss = 0\n",
    "        for anchor_index in np.arange(data.shape[0]):\n",
    "            anchor_label = targets[anchor_index]\n",
    "            # Sample a positive sample from our total dataset\n",
    "            indices_of_positve_samples = torch.where(actual_labels == anchor_label)[0]\n",
    "            positive_sample_index = torch.randint(0, indices_of_positve_samples.shape[0], size=(1,))\n",
    "            positive_sample_index_list.append(positive_sample_index.item())\n",
    "\n",
    "            # Sample negative samples from our total dataset. We will use a \n",
    "            # weighted probability distribution. There will be a 0.0001 probability\n",
    "            # that we will sample from the positive class and a (1-0.001)/(K-1)\n",
    "            # probability that we sample from the remaining K-1 classes\n",
    "\n",
    "            # Create a tensor with custom probabilities\n",
    "            epsilon = 0.0001\n",
    "            probs = torch.zeros((1, unique_syllables.shape[0]))\n",
    "            probs[:,:] = (1-epsilon)/(probs.shape[1]-1)\n",
    "            probs[:,int(anchor_label)] = epsilon\n",
    "\n",
    "\n",
    "            # Number of samples to generate\n",
    "            num_samples = unique_syllables.shape[0] - 1\n",
    "\n",
    "            # Sample indices based on the custom probabilities\n",
    "            sampled_labels = torch.multinomial(probs, num_samples, replacement=False)\n",
    "\n",
    "            # Now let's randomly sample an index value from each sampled label\n",
    "\n",
    "            random_samples = {key: np.random.choice(values, num_negative_samples_each) for key, values in indices_dict.items() if key in sampled_labels}\n",
    "            indices_of_negative_samples = np.array(list(random_samples.values()))\n",
    "            negative_samples_indices_list.append(indices_of_negative_samples)\n",
    "            a = np.stack(indices_of_negative_samples)\n",
    "            indices_of_negative_samples = np.stack(indices_of_negative_samples).reshape(a.shape[0]*a.shape[1],)\n",
    "\n",
    "\n",
    "            # Now let's extract the positive and negative samples' spectrogram \n",
    "            # slices\n",
    "\n",
    "            positive_sample = stacked_windows_tensor[positive_sample_index, :,:,:]\n",
    "            negative_samples = stacked_windows_tensor[indices_of_negative_samples, :,:,:]\n",
    "\n",
    "            dat = torch.concatenate((positive_sample, negative_samples))\n",
    "\n",
    "            artificial_labels = torch.zeros((1,1 + (unique_syllables.shape[0]-1)*num_negative_samples_each))\n",
    "            artificial_labels[:,0] = 1\n",
    "\n",
    "            # Get the number of rows in the tensors\n",
    "            num_rows = dat.shape[0]\n",
    "\n",
    "            # Generate a random permutation of indices\n",
    "            shuffled_indices_pseudo = torch.randperm(num_rows)\n",
    "\n",
    "            # Shuffle both tensors based on the same indices\n",
    "            dat = dat[shuffled_indices_pseudo,:,:,:].to(device)\n",
    "            artificial_labels = artificial_labels[:,shuffled_indices_pseudo].to(device)\n",
    "\n",
    "            pred_probs = cnn_model(dat)\n",
    "\n",
    "            # h_t = 1/(1+torch.exp(-1*(torch.log(pred_probs) - torch.log(probs).T)))\n",
    "            # torch.sum(artificial_labels.T*torch.log(h_t) + (1-artificial_labels.T)*torch.log(h_t))\n",
    "\n",
    "            loss = criterion(pred_probs, artificial_labels.T.double())\n",
    "            batch_loss+=loss\n",
    "\n",
    "        total_batch_loss_list.append(batch_loss.item())\n",
    "        # print(f'Batch Loss: {batch_loss.item():.4f}')\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    mean_batch_loss_per_epoch = np.mean(total_batch_loss_list)\n",
    "    mean_batch_loss_per_epoch_list.append(mean_batch_loss_per_epoch)\n",
    "    print(\"=================================================================\")\n",
    "    print(f'Epoch {epoch}, Mean Batch Loss: {mean_batch_loss_per_epoch:.4f}')\n",
    "    print(\"=================================================================\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"NCE Loss\")\n",
    "plt.plot(mean_batch_loss_per_epoch_list)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push our training data through the model to obtain an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.to('cpu').eval()\n",
    "model_embedding_arr = torch.empty((0,1000)).to('cpu')\n",
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    x = cnn_model.conv1(data.to('cpu'))\n",
    "    x = cnn_model.relu(x)\n",
    "    x = cnn_model.pool1(x)\n",
    "    x = cnn_model.conv2(x)\n",
    "    x = cnn_model.relu(x)\n",
    "    x = cnn_model.pool2(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = cnn_model.fc1(x)\n",
    "    model_embedding_arr = torch.concatenate((model_embedding_arr, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17811, 1000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_embedding_arr.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a UMAP on our training data's embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP()\n",
    "embedding_umap = reducer.fit_transform(model_embedding_arr.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_file, save\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9552,  7910, 12812, ..., 23866,  4790, 14288])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_indices[:split_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_colors_per_minispec_training = mean_colors_per_minispec[shuffled_indices[:split_point],:]\n",
    "mean_colors_per_minispec_testing = mean_colors_per_minispec[shuffled_indices[split_point:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehUserWarning: ColumnDataSource's columns must be of the same length. Current lengths: ('colors', 17811), ('image', 0), ('x', 17811), ('y', 17811)\n"
     ]
    }
   ],
   "source": [
    "# Specify an HTML file to save the Bokeh image to.\n",
    "output_file(filename='umap_of_TweetyNet_Embed_NCE.html')\n",
    "\n",
    "# Convert the UMAP embedding to a Pandas Dataframe\n",
    "spec_df = pd.DataFrame(embedding_umap, columns=('x', 'y'))\n",
    "\n",
    "\n",
    "# Create a ColumnDataSource from the data. This contains the UMAP embedding components and the mean colors per mini-spectrogram\n",
    "source = ColumnDataSource(data=dict(x = embedding_umap[:,0], y = embedding_umap[:,1], colors=mean_colors_per_minispec_training))\n",
    "\n",
    "\n",
    "# Create a figure and add a scatter plot\n",
    "p = figure(width=800, height=600, tools=('pan, box_zoom, hover, reset'))\n",
    "p.scatter(x='x', y='y', size = 7, color = 'colors', source=source)\n",
    "\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = \"\"\"\n",
    "    <div>\n",
    "        <h3>@x, @y</h3>\n",
    "        <div>\n",
    "            <img\n",
    "                src=\"@image\" height=\"100\" alt=\"@image\" width=\"100\"\n",
    "                style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                border=\"2\"\n",
    "            ></img>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=\"\"\"\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# Set the image path for each data point\n",
    "# source.data['image'] = []\n",
    "# for i in np.arange(spec_df.shape[0]):\n",
    "#     source.data['image'].append(f'{folderpath_song}/Plots/Window_Plots/Window_{shuffled_indices[split_point:]}.png')\n",
    "\n",
    "show(p)\n",
    "\n",
    "# save(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9552,  7910, 12812, ..., 23866,  4790, 14288])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_indices[:split_point]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply UMAP to the raw training data spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer_umap_alone = umap.UMAP()\n",
    "embedding_umap_alone = reducer_umap_alone.fit_transform(train_data_tensor.cpu().detach().numpy().reshape(train_data_tensor.shape[0],-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17811, 4000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tensor.cpu().detach().numpy().reshape(train_data_tensor.shape[0],-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an HTML file to save the Bokeh image to.\n",
    "output_file(filename='umap_alone.html')\n",
    "\n",
    "# Convert the UMAP embedding to a Pandas Dataframe\n",
    "spec_df = pd.DataFrame(embedding_umap_alone, columns=('x', 'y'))\n",
    "\n",
    "\n",
    "# Create a ColumnDataSource from the data. This contains the UMAP embedding components and the mean colors per mini-spectrogram\n",
    "source = ColumnDataSource(data=dict(x = embedding_umap_alone[:,0], y = embedding_umap_alone[:,1], colors=mean_colors_per_minispec_training))\n",
    "\n",
    "\n",
    "# Create a figure and add a scatter plot\n",
    "p = figure(width=800, height=600, tools=('pan, box_zoom, hover, reset'))\n",
    "p.scatter(x='x', y='y', size = 7, color = 'colors', source=source)\n",
    "\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = \"\"\"\n",
    "    <div>\n",
    "        <h3>@x, @y</h3>\n",
    "        <div>\n",
    "            <img\n",
    "                src=\"@image\" height=\"100\" alt=\"@image\" width=\"100\"\n",
    "                style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                border=\"2\"\n",
    "            ></img>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=\"\"\"\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# Set the image path for each data point\n",
    "# source.data['image'] = []\n",
    "# for i in np.arange(spec_df.shape[0]):\n",
    "#     source.data['image'].append(f'{songpath}/Plots/Window_Plots/Window_{shuffled_indices[split_point:[i]]}.png')\n",
    "\n",
    "show(p)\n",
    "\n",
    "# save(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push our testing data through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.to('cpu').eval()\n",
    "model_embedding_arr_testing = torch.empty((0,1000)).to('cpu')\n",
    "for batch_idx, (data, targets) in enumerate(test_loader):\n",
    "    x = cnn_model.conv1(data.to('cpu'))\n",
    "    x = cnn_model.relu(x)\n",
    "    x = cnn_model.pool1(x)\n",
    "    x = cnn_model.conv2(x)\n",
    "    x = cnn_model.relu(x)\n",
    "    x = cnn_model.pool2(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = cnn_model.fc1(x)\n",
    "    model_embedding_arr_testing = torch.concatenate((model_embedding_arr_testing, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP()\n",
    "embedding_umap_testing = reducer.fit_transform(model_embedding_arr_testing.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an HTML file to save the Bokeh image to.\n",
    "output_file(filename='umap_of_TweetyNet_Embed_NCE_testing.html')\n",
    "\n",
    "# Convert the UMAP embedding to a Pandas Dataframe\n",
    "spec_df = pd.DataFrame(embedding_umap_testing, columns=('x', 'y'))\n",
    "\n",
    "\n",
    "# Create a ColumnDataSource from the data. This contains the UMAP embedding components and the mean colors per mini-spectrogram\n",
    "source = ColumnDataSource(data=dict(x = embedding_umap_testing[:,0], y = embedding_umap_testing[:,1], colors=mean_colors_per_minispec_testing))\n",
    "\n",
    "\n",
    "# Create a figure and add a scatter plot\n",
    "p = figure(width=800, height=600, tools=('pan, box_zoom, hover, reset'))\n",
    "p.scatter(x='x', y='y', size = 7, color = 'colors', source=source)\n",
    "\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = \"\"\"\n",
    "    <div>\n",
    "        <h3>@x, @y</h3>\n",
    "        <div>\n",
    "            <img\n",
    "                src=\"@image\" height=\"100\" alt=\"@image\" width=\"100\"\n",
    "                style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                border=\"2\"\n",
    "            ></img>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=\"\"\"\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# Set the image path for each data point\n",
    "# source.data['image'] = []\n",
    "# for i in np.arange(spec_df.shape[0]):\n",
    "#     source.data['image'].append(f'{songpath}/Plots/Window_Plots/Window_{i}.png')\n",
    "\n",
    "show(p)\n",
    "\n",
    "# save(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7634, 1, 100, 40])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17811, 1, 100, 40])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25445"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7634 + 17811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3000196502259776"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7634/25445"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quantify how well the model classifies syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97446\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of SVC with a linear kernel\n",
    "svc_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_classifier.fit(embedding_umap_testing, test_labels)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svc_classifier.predict(embedding_umap_testing)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
