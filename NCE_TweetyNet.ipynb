{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# import umap\n",
    "# Parameters we set\n",
    "# num_spec = 1\n",
    "# window_size = 100\n",
    "# stride = 10\n",
    "\n",
    "folderpath_song = '/home/akapoor/Dropbox (University of Oregon)/Kapoor_Ananya/01_Projects/01_b_Canary_SSL/Canary_SSL_Repo/num_songs_10_num_syllables_10_phrase_repeats_5_radius_0.01/'\n",
    "\n",
    "stacked_windows = np.load(f'{folderpath_song}stacked_windows.npy')\n",
    "stacked_window_times = np.load(f'{folderpath_song}stacked_window_times.npy')\n",
    "stacked_labels_for_window = np.load(f'{folderpath_song}labels_for_window.npy')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda:0\"\n",
    "else:\n",
    "  device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define neural network\n",
    "\n",
    "\n",
    "class TweetyNetCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=32, kernel_size=(5, 5), stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(8, 1), stride=(8, 1))\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=(5, 5), stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(8, 1), stride=(8, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(64*1*36, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        # x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "cnn_model = TweetyNetCNN()\n",
    "cnn_model = cnn_model.double().to(device)\n",
    "\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Extract the first batch\n",
    "\n",
    "stacked_windows_tensor = stacked_windows.copy()\n",
    "stacked_windows_tensor.shape = (stacked_windows.shape[0], 1, 100, 40)\n",
    "stacked_windows_tensor = torch.tensor(stacked_windows_tensor).double()\n",
    "\n",
    "\n",
    "actual_labels = np.max(stacked_labels_for_window, axis=1)\n",
    "actual_labels = torch.tensor(actual_labels)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Choose a random 64 indices from our total data\n",
    "\n",
    "# batch_indices = np.random.randint(0, stacked_windows_tensor.shape[0], batch_size)\n",
    "\n",
    "# x = stacked_windows_tensor[batch_indices, :,:,:]\n",
    "# y = actual_labels[batch_indices]\n",
    "\n",
    "# # Create a TensorDataset to combine features and labels\n",
    "dataset = TensorDataset(stacked_windows_tensor, actual_labels)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Extract the first batch\n",
    "# for batch in train_loader:\n",
    "#     first_batch = batch\n",
    "#     break  # Exit the loop after the first iteration\n",
    "\n",
    "# x = first_batch[0]\n",
    "# y = first_batch[1]\n",
    "\n",
    "positive_sample_index_list = []\n",
    "negative_samples_indices_list = []\n",
    "\n",
    "\n",
    "unique_syllables = torch.unique(actual_labels)\n",
    "\n",
    "indices_dict = {int(element): np.where(actual_labels == element)[0] for element in unique_syllables}\n",
    "num_negative_samples_each = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 45.4311\n",
      "Loss: 23.2163\n",
      "Loss: 3.4281\n",
      "Loss: 2.2021\n",
      "Loss: 3.6960\n",
      "Loss: 5.1974\n",
      "Loss: 6.4412\n",
      "Loss: 7.2821\n",
      "Loss: 7.6672\n",
      "Loss: 7.6249\n",
      "Loss: 7.2559\n",
      "Loss: 6.6183\n",
      "Loss: 5.7993\n",
      "Loss: 4.8646\n",
      "Loss: 3.8830\n",
      "Loss: 2.9049\n",
      "Loss: 2.0279\n",
      "Loss: 2.3454\n",
      "Loss: 4.2415\n",
      "Loss: 2.0428\n",
      "Loss: 1.8624\n",
      "Loss: 2.1414\n",
      "Loss: 2.4052\n",
      "Loss: 2.5786\n",
      "Loss: 2.6543\n",
      "Loss: 2.6397\n",
      "Loss: 2.5486\n",
      "Loss: 2.3993\n",
      "Loss: 2.2137\n",
      "Loss: 2.0230\n",
      "Loss: 1.8685\n",
      "Loss: 1.8206\n",
      "Loss: 1.9294\n",
      "Loss: 2.0689\n",
      "Loss: 2.1058\n",
      "Loss: 2.0180\n",
      "Loss: 1.8877\n",
      "Loss: 1.8198\n",
      "Loss: 1.8406\n",
      "Loss: 1.8934\n",
      "Loss: 1.9205\n",
      "Loss: 1.9118\n",
      "Loss: 1.8770\n",
      "Loss: 1.8343\n",
      "Loss: 1.8193\n",
      "Loss: 1.8497\n",
      "Loss: 1.8398\n",
      "Loss: 1.8217\n",
      "Loss: 1.8178\n",
      "Loss: 1.8228\n",
      "Loss: 1.8280\n",
      "Loss: 1.8297\n",
      "Loss: 1.8251\n",
      "Loss: 1.8200\n",
      "Loss: 1.8176\n",
      "Loss: 1.8193\n",
      "Loss: 1.8228\n",
      "Loss: 1.8219\n",
      "Loss: 1.8181\n",
      "Loss: 1.8200\n",
      "Loss: 1.8197\n",
      "Loss: 1.8176\n",
      "Loss: 1.8181\n",
      "Loss: 1.8191\n",
      "Loss: 1.8187\n",
      "Loss: 1.8178\n",
      "Loss: 1.8176\n",
      "Loss: 1.8181\n",
      "Loss: 1.8184\n",
      "Loss: 1.8182\n",
      "Loss: 1.8178\n",
      "Loss: 1.8176\n",
      "Loss: 1.8177\n",
      "Loss: 1.8178\n",
      "Loss: 1.8178\n",
      "Loss: 1.8177\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8177\n",
      "Loss: 1.8177\n",
      "Loss: 1.8177\n",
      "Loss: 1.8177\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8175\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.8176\n",
      "Loss: 1.0508\n"
     ]
    }
   ],
   "source": [
    "# %% Train Neural network \n",
    "\n",
    "cnn_model.train()\n",
    "num_epoch = 100\n",
    "total_batch_loss_list = []\n",
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    batch_loss = 0\n",
    "    for anchor_index in np.arange(data.shape[0]):\n",
    "        anchor_label = targets[anchor_index]\n",
    "        # Sample a positive sample from our total dataset\n",
    "        indices_of_positve_samples = torch.where(actual_labels == anchor_label)[0]\n",
    "        positive_sample_index = torch.randint(0, indices_of_positve_samples.shape[0], size=(1,))\n",
    "        positive_sample_index_list.append(positive_sample_index.item())\n",
    "        \n",
    "        # Sample negative samples from our total dataset. We will use a \n",
    "        # weighted probability distribution. There will be a 0.0001 probability\n",
    "        # that we will sample from the positive class and a (1-0.001)/(K-1)\n",
    "        # probability that we sample from the remaining K-1 classes\n",
    "        \n",
    "        # Create a tensor with custom probabilities\n",
    "        epsilon = 0.0001\n",
    "        probs = torch.zeros((1, unique_syllables.shape[0]))\n",
    "        probs[:,:] = (1-epsilon)/(probs.shape[1]-1)\n",
    "        probs[:,int(anchor_label)] = epsilon\n",
    "\n",
    "        \n",
    "        # Number of samples to generate\n",
    "        num_samples = unique_syllables.shape[0] - 1\n",
    "        \n",
    "        # Sample indices based on the custom probabilities\n",
    "        sampled_labels = torch.multinomial(probs, num_samples, replacement=False)\n",
    "        \n",
    "        # Now let's randomly sample an index value from each sampled label\n",
    "        \n",
    "        random_samples = {key: np.random.choice(values, num_negative_samples_each) for key, values in indices_dict.items() if key in sampled_labels}\n",
    "        indices_of_negative_samples = np.array(list(random_samples.values()))\n",
    "        negative_samples_indices_list.append(indices_of_negative_samples)\n",
    "        a = np.stack(indices_of_negative_samples)\n",
    "        indices_of_negative_samples = np.stack(indices_of_negative_samples).reshape(a.shape[0]*a.shape[1],)\n",
    "\n",
    "    \n",
    "        # Now let's extract the positive and negative samples' spectrogram \n",
    "        # slices\n",
    "        \n",
    "        positive_sample = stacked_windows_tensor[positive_sample_index, :,:,:]\n",
    "        negative_samples = stacked_windows_tensor[indices_of_negative_samples, :,:,:]\n",
    "        \n",
    "        dat = torch.concatenate((positive_sample, negative_samples))\n",
    "        \n",
    "        artificial_labels = torch.zeros((1,1 + (unique_syllables.shape[0]-1)*num_negative_samples_each))\n",
    "        artificial_labels[:,0] = 1\n",
    "        \n",
    "        # Get the number of rows in the tensors\n",
    "        num_rows = dat.shape[0]\n",
    "        \n",
    "        # Generate a random permutation of indices\n",
    "        shuffled_indices = torch.randperm(num_rows)\n",
    "        \n",
    "        # Shuffle both tensors based on the same indices\n",
    "        dat = dat[shuffled_indices,:,:,:].to(device)\n",
    "        artificial_labels = artificial_labels[:,shuffled_indices].to(device)\n",
    "        \n",
    "        pred_probs = cnn_model(dat)\n",
    "        \n",
    "        # h_t = 1/(1+torch.exp(-1*(torch.log(pred_probs) - torch.log(probs).T)))\n",
    "        # torch.sum(artificial_labels.T*torch.log(h_t) + (1-artificial_labels.T)*torch.log(h_t))\n",
    "        \n",
    "        loss = criterion(pred_probs, artificial_labels.T.double())\n",
    "        batch_loss+=loss\n",
    "        \n",
    "    total_batch_loss_list.append(batch_loss.item())\n",
    "    print(f'Loss: {batch_loss.item():.4f}')\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4c5c2d81f0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(total_batch_loss_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push our data through the model to obtain an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.to('cpu').eval()\n",
    "model_embedding_arr = torch.empty((0,1000)).to('cpu')\n",
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    x = cnn_model.conv1(data.to('cpu'))\n",
    "    x = cnn_model.relu(x)\n",
    "    x = cnn_model.pool1(x)\n",
    "    x = cnn_model.conv2(x)\n",
    "    x = cnn_model.relu(x)\n",
    "    x = cnn_model.pool2(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = cnn_model.fc1(x)\n",
    "    model_embedding_arr = torch.concatenate((model_embedding_arr, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25445, 1000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_embedding_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP()\n",
    "embedding_umap = reducer.fit_transform(model_embedding_arr.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters we set\n",
    "num_spec = 10\n",
    "window_size = 100\n",
    "stride = 10\n",
    "\n",
    "\n",
    "phrase_repeats = 5\n",
    "num_songs = 10\n",
    "radius_value = 0.01\n",
    "num_syllables = 10\n",
    "\n",
    "folderpath = '/home/akapoor/Dropbox (University of Oregon)/Kapoor_Ananya/01_Projects/01_b_Canary_SSL/Canary_SSL_Repo/'\n",
    "songpath = f'{folderpath}num_songs_{num_songs}_num_syllables_{num_syllables}_phrase_repeats_{phrase_repeats}_radius_{radius_value}/'\n",
    "\n",
    "mean_colors_per_minispec = np.load(f'{songpath}mean_colors_per_minispec.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_file, save\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_colors_per_minispec_batch = mean_colors_per_minispec[batch_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehUserWarning: ColumnDataSource's columns must be of the same length. Current lengths: ('colors', 25445), ('image', 0), ('x', 25445), ('y', 25445)\n"
     ]
    }
   ],
   "source": [
    "# Specify an HTML file to save the Bokeh image to.\n",
    "output_file(filename='umap_of_TweetyNet_Embed_NCE.html')\n",
    "\n",
    "# Convert the UMAP embedding to a Pandas Dataframe\n",
    "spec_df = pd.DataFrame(embedding_umap, columns=('x', 'y'))\n",
    "\n",
    "\n",
    "# Create a ColumnDataSource from the data. This contains the UMAP embedding components and the mean colors per mini-spectrogram\n",
    "source = ColumnDataSource(data=dict(x = embedding_umap[:,0], y = embedding_umap[:,1], colors=mean_colors_per_minispec))\n",
    "\n",
    "\n",
    "# Create a figure and add a scatter plot\n",
    "p = figure(width=800, height=600, tools=('pan, box_zoom, hover, reset'))\n",
    "p.scatter(x='x', y='y', size = 7, color = 'colors', source=source)\n",
    "\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = \"\"\"\n",
    "    <div>\n",
    "        <h3>@x, @y</h3>\n",
    "        <div>\n",
    "            <img\n",
    "                src=\"@image\" height=\"100\" alt=\"@image\" width=\"100\"\n",
    "                style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                border=\"2\"\n",
    "            ></img>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=\"\"\"\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# Set the image path for each data point\n",
    "source.data['image'] = []\n",
    "for i in np.arange(spec_df.shape[0]):\n",
    "    source.data['image'].append(f'{songpath}/Plots/Window_Plots/Window_{i}.png')\n",
    "\n",
    "show(p)\n",
    "\n",
    "# save(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045],\n",
       "        [0.0045]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = stacked_windows_tensor[batch_indices, :,:,:]\n",
    "x = x.reshape(32, x.shape[1]*x.shape[2]*x.shape[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "umap_alone = reducer.fit_transform(x.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehUserWarning: ColumnDataSource's columns must be of the same length. Current lengths: ('colors', 32), ('image', 0), ('x', 32), ('y', 32)\n"
     ]
    }
   ],
   "source": [
    "# Specify an HTML file to save the Bokeh image to.\n",
    "output_file(filename='umap_alone.html')\n",
    "\n",
    "# Convert the UMAP embedding to a Pandas Dataframe\n",
    "spec_df = pd.DataFrame(umap_alone, columns=('x', 'y'))\n",
    "\n",
    "\n",
    "# Create a ColumnDataSource from the data. This contains the UMAP embedding components and the mean colors per mini-spectrogram\n",
    "source = ColumnDataSource(data=dict(x = umap_alone[:,0], y = umap_alone[:,1], colors=mean_colors_per_minispec_batch))\n",
    "\n",
    "\n",
    "# Create a figure and add a scatter plot\n",
    "p = figure(width=800, height=600, tools=('pan, box_zoom, hover, reset'))\n",
    "p.scatter(x='x', y='y', size = 7, color = 'colors', source=source)\n",
    "\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = \"\"\"\n",
    "    <div>\n",
    "        <h3>@x, @y</h3>\n",
    "        <div>\n",
    "            <img\n",
    "                src=\"@image\" height=\"100\" alt=\"@image\" width=\"100\"\n",
    "                style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                border=\"2\"\n",
    "            ></img>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=\"\"\"\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# Set the image path for each data point\n",
    "source.data['image'] = []\n",
    "for i in np.arange(spec_df.shape[0]):\n",
    "    source.data['image'].append(f'{songpath}/Plots/Window_Plots/Window_{batch_indices[i]}.png')\n",
    "\n",
    "show(p)\n",
    "\n",
    "# save(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# %% Implementation of the NCC classifier \n",
    "\n",
    "actual_labels = np.max(stacked_labels_for_window, axis = 1)\n",
    "\n",
    "unique_labels = np.unique(actual_labels)\n",
    "\n",
    "avg_representation = np.zeros((unique_labels.shape[0], 2)) # 2nd dimension is UMAP embedding size\n",
    "\n",
    "for lab in unique_labels:\n",
    "    lab = int(lab)\n",
    "    embedding_rows = np.where(actual_labels == lab)\n",
    "    embedding_subset = np.squeeze(embedding_umap[embedding_rows, :])\n",
    "    avg_representation[lab, :] = np.mean(embedding_subset, axis = 0)\n",
    "    \n",
    "pred_labels = []\n",
    "for i in np.arange(embedding_umap.shape[0]):\n",
    "    dist_metric = np.sum((embedding_umap[i,:] - avg_representation)**2, axis = 1)\n",
    "    pred_labels.append(np.argmin(dist_metric))\n",
    "    \n",
    "\n",
    "acc_value = np.mean(actual_labels == pred_labels)\n",
    "\n",
    "# This shows that the representations of syllables form centroid like geometry\n",
    "# in representation space. \n",
    "\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "\n",
    "v_measure_score(actual_labels, np.array(pred_labels))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of SVC with a linear kernel\n",
    "svc_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_classifier.fit(embedding_umap, actual_labels)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svc_classifier.predict(embedding_umap)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(actual_labels, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8389776104293857"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_measure_score(actual_labels, np.array(pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer_umap_alone = umap.UMAP()\n",
    "embedding_umap_alone = reducer_umap_alone.fit_transform(stacked_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehUserWarning: ColumnDataSource's columns must be of the same length. Current lengths: ('colors', 25445), ('image', 0), ('x', 25445), ('y', 25445)\n"
     ]
    }
   ],
   "source": [
    "# Specify an HTML file to save the Bokeh image to.\n",
    "output_file(filename='umap_alone.html')\n",
    "\n",
    "# Convert the UMAP embedding to a Pandas Dataframe\n",
    "spec_df = pd.DataFrame(embedding_umap_alone, columns=('x', 'y'))\n",
    "\n",
    "\n",
    "# Create a ColumnDataSource from the data. This contains the UMAP embedding components and the mean colors per mini-spectrogram\n",
    "source = ColumnDataSource(data=dict(x = embedding_umap_alone[:,0], y = embedding_umap_alone[:,1], colors=mean_colors_per_minispec))\n",
    "\n",
    "\n",
    "# Create a figure and add a scatter plot\n",
    "p = figure(width=800, height=600, tools=('pan, box_zoom, hover, reset'))\n",
    "p.scatter(x='x', y='y', size = 7, color = 'colors', source=source)\n",
    "\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = \"\"\"\n",
    "    <div>\n",
    "        <h3>@x, @y</h3>\n",
    "        <div>\n",
    "            <img\n",
    "                src=\"@image\" height=\"100\" alt=\"@image\" width=\"100\"\n",
    "                style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                border=\"2\"\n",
    "            ></img>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=\"\"\"\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# Set the image path for each data point\n",
    "source.data['image'] = []\n",
    "for i in np.arange(spec_df.shape[0]):\n",
    "    source.data['image'].append(f'{songpath}/Plots/Window_Plots/Window_{i}.png')\n",
    "\n",
    "show(p)\n",
    "\n",
    "# save(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
